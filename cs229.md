## Neural Networks and deep learning
loss function的选择：
凸优化，optimization problem that is convex
对于logistic常用-(ylogy^ + (1-y)log(1-y^))
cost function是所有样本loss function 的平均
linear classifier的训练过程是调整w和b不断使cost function减小
梯度下降法所要做的是从初始点开始，朝最陡的下坡方向走一步，接近全局最优解
表达成
```
repeat {
    w:=w-alpha*dJ/dw
    b:=b-alpha*dJ/db
}
```
:=是更新，alpha是学习率，在收敛之前不断重复这个过程，d/d是导数，也是曲线w-J在该点的斜率（高除以宽，逼近的那个小三角形），delta偏导，d一元导
前馈：L(loss function)对各变量求导。
z = w1x1 + w2x2 + b -> a = sigma(Z) -> L(a, y)
"dz" = dL/dz = dL/da * da/dz <- "da" = da / dL
再求出"dw1" = dL/dw1 = x1dz -> w1 = w1 - alpha乘"dw"

## 改善深层神经网络：超参数调试、正则化以及优化
